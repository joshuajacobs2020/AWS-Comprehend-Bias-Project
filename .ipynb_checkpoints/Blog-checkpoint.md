# AWS Comprehend
### Ved Udare, Joshua Jacobs, Will Coupe, Eugene Lim, Austin Cherian

### Problem definition

##### Does AWS’s Comprehend have inherent political bias? 


### Abstract 


AWS Comprehend is a natural language processing (NLP) service provided by Amazon Web Services (AWS). It allows users to extract insights and meaning from unstructured text using machine learning algorithms. Two important key functions amongst a host of others of Comprehend are entity recognition, the ability to identify entities such as people, places, and organizations in a given text, and sentiment analysis, the ability to analyze the sentiment of a given text as either positive, negative, or neutral. In this study, we aim to investigate whether AWS Comprehend exhibits inherent political bias in its sentiment analysis feature. We plan to assess the association between political parties and Comprehend’s sentiment analysis scores by substituting different members of Congress into a political statement.


### Background 
The increasing use of artificial intelligence and machine learning in various aspects of society has raised concerns about political bias in computer algorithms. This bias can shape public opinion and influence people's beliefs and behaviors. For example, search engine result bias has been a significant problem in society, as it can shape public opinion and influence people's beliefs and behaviors. According to a study by Pew Research Center, about 93% of online experiences begin with a search engine, and search engine results can have a significant impact on people's perceptions of various issues. However, there have been concerns about political bias in search engine results. For example, during the 2016 U.S. Presidential election, some people claimed that Google's search results were biased in favor of Hillary Clinton (The Washington Post, 2016). Search engine result bias can also perpetuate stereotypes and limit opportunities for certain groups. According to a report by the Open Society Foundations, search engine algorithms can amplify stereotypes and reinforce prejudice against minority groups, leading to limited access to information and resources (Open Society Foundations, 2018). As search engines become increasingly important sources of information, it's essential that they remain impartial and free from any political bias to ensure that they provide accurate and unbiased information to users.


In addition, social media content moderation bias has become a growing concern in society, as social media platforms like Facebook and Twitter have become a primary source of news and information for many people. According to a survey by Pew Research Center, about 55% of American adults get news from social media (Pew Research Center, 2020). However, there have been concerns about political bias in social media content moderation policies. Some people claim that these platforms unfairly censor conservative voices, while others argue that they allow too much hate speech and misinformation to spread. According to a report by the non-profit organization PEN America, social media platforms have failed to effectively combat hate speech and disinformation, which has led to the spread of dangerous and false information (PEN America, 2019). Social media content moderation bias can also have a chilling effect on free speech, as people may be hesitant to express their opinions or share information for fear of being censored or punished. As social media continues to play a significant role in public discourse, it's important that these platforms implement fair and transparent content moderation policies that are free from political bias.


According to Rozado (2022), ChatGPT is a large language model trained by OpenAI based on the GPT-3.5 architecture, and he attempts to assess ChatGPT's political leanings by feeding it several political orientation tests such as “Pew Research Political Typology Quiz,” “Political Compass Test,” “World’s Smallest Political Quiz,” and the “Political Spectrum Quiz.” The author notes that the quizzes contain questions that cover a range of topics, including economics, social issues, foreign policy, and more, and ChatGPT's responses to the questions were analyzed to determine where it might fall on the political compass. Rozado consistently found across all tests that ChatGPT’s answers to political orientation quizzes were left-leaning. The results of his assessment are detailed in the following visual (Rozado, 2022).

![Results Visuals](https://github.com/joshuajacobs2020/QTM-350---Group-4/raw/main/results.png)


Rozado (2022) acknowledges that ChatGPT is a machine-learning model and, therefore, does not have political beliefs or opinions of its own. However, he notes that the responses generated by ChatGPT suggest that it was "trained on content containing political biases." ChatGPT, like many artificial intelligence systems, is trained on a copious amount of textual data from the internet. Additionally, Rozado (2022) highlights the possibility that "a team of human raters was involved in judging the quality of the model’s answers" and the political bias of the human raters, assuming that they were not a representative sample of the larger public, could have also influenced the political leanings of ChatGPT.


Overall, the article provides an interesting insight into how machine learning models like ChatGPT might reflect the political views present in the data they were trained on. However, the author also notes that this does not necessarily mean that ChatGPT or other machine learning models have political beliefs of their own. When algorithms are biased, they can have a significant impact on society. It can reinforce existing prejudices and stereotypes, limit opportunities for certain groups, and perpetuate inequality. Biased algorithms can also undermine public trust in institutions that use them, such as government agencies, law enforcement, and media outlets. As algorithms become more prevalent in our daily lives, it's important that they are transparent and accountable and that measures are taken to ensure that they are free from any political bias. The findings of Rozado (2022) raise the question of whether other AIs like AWS Comprehend also have political leanings. As a result, we aim to do a similar analysis of AWS’s ML, Comprehend, to see if it contains any political bias as well. 


### Methods
We imported a data file from Every Politician (https://everypolitician.org/united-states-of-america/senate/download.html) that contains the names of every Congress member in the 116th Congress (2019) and information about them (social media accounts, state they represent, gender, etc.) 
We cleaned the data only to include the names of Congressmen, their listed political party affiliations, and their gender. 
We then converted the political parties into binary (Democrat=1, Republican=0) and gender into binary (female=1, male=0). 
Then, we created a sentiment function using the sentence “{entity} claims they want to protect children from the dangers of social media"
Next, we replaced “{entity}” with the names of each member of Congress, ran a sentiment analysis, and tabulated all the sentiment analysis scores. The sentiment analysis scores included 4 sentiments on a 0-1 scale: a positive, negative, mixed, and neutral score. Additionally, it contained an overall score that filtered for the sentiment with the highest score.


### Analysis
We created box and whisker plots of neutral scores for each political party and repeated this process with positive and negative scores. 

#### Regression Results
(1) Non-Neutral Scores
(2) Positive Scores
(3) Mixed Scores
(4) Negative Scores

<table style="text-align:center"><tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left"></td><tr><td style="text-align:left"></td><td>(1)</td><td>(2)</td><td>(3)</td><td>(4)</td></tr><tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align:left">Democratic</td><td>0.009<sup></sup></td><td>-0.000<sup></sup></td><td>0.001<sup></sup></td><td>0.008<sup></sup></td></tr><tr><td style="text-align:left"></td><td>(0.015)</td><td>(0.002)</td><td>(0.002)</td><td>(0.015)</td></tr><tr><td style="text-align:left">Intercept</td><td>0.103<sup>***</sup></td><td>0.012<sup>***</sup></td><td>0.004<sup>***</sup></td><td>0.088<sup>***</sup></td></tr><tr><td style="text-align:left"></td><td>(0.015)</td><td>(0.002)</td><td>(0.001)</td><td>(0.015)</td></tr><tr><td style="text-align:left">Female</td><td>-0.002<sup></sup></td><td>-0.004<sup>***</sup></td><td>0.003<sup></sup></td><td>-0.002<sup></sup></td></tr><tr><td style="text-align:left"></td><td>(0.015)</td><td>(0.001)</td><td>(0.003)</td><td>(0.014)</td></tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align: left">Observations</td><td>98</td><td>98</td><td>98</td><td>98</td></tr><tr><td style="text-align: left">R<sup>2</sup></td><td>0.006</td><td>0.067</td><td>0.015</td><td>0.005</td></tr><tr><td style="text-align: left">Adjusted R<sup>2</sup></td><td>-0.015</td><td>0.048</td><td>-0.006</td><td>-0.016</td></tr><tr><td style="text-align: left">Residual Std. Error</td><td>0.065 (df=95)</td><td>0.006 (df=95)</td><td>0.012 (df=95)</td><td>0.061 (df=95)</td></tr><tr><td style="text-align: left">F Statistic</td><td>0.321<sup></sup> (df=2; 95)</td><td>6.959<sup>***</sup> (df=2; 95)</td><td>1.060<sup></sup> (df=2; 95)</td><td>0.304<sup></sup> (df=2; 95)</td></tr><tr><td colspan="5" style="border-bottom: 1px solid black"></td></tr><tr><td style="text-align: left">Note:</td> <td colspan="4" style="text-align: right">  <sup>*</sup>p&lt;0.1;  <sup>**</sup>p&lt;0.05;  <sup>***</sup>p&lt;0.01 </td></tr></table>


### Conclusions



References:
Pew Research Center. (2021). About Pew Research Center. Retrieved from https://www.pewresearch.org/about/


The Washington Post. (2016). Why Google is being accused of pro-Clinton bias. Retrieved from https://www.washingtonpost.com/news/the-switch/wp/2016/06/10/why-google-is-being-accused-of-pro-clinton-bias/


Open Society Foundations. (2018). How search engines reinforce racism. Retrieved from https://www.opensocietyfoundations.org/voices/how-search-engines-reinforce-racism




Pew Research Center. (2020). Social media usage in 2020. Retrieved from https://www.pewresearch.org/internet/2021/04/07/social-media-use-in-2021/




PEN America. (2019). False equation: the dangers of “free speech absolutism”. Retrieved from https://pen.org/false-equation-the-dangers-of-free-speech-absolutism/
Rozado, D. (2022, December 13). Where Does ChatGPT Fall on the Political Compass? Reason. Retrieved from https://reason.com/2022/12/13/where-does-chatgpt-fall-on-the-political-compass/
