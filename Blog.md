# AWS Comprehend
### Ved Udare, Joshua Jacobs, Will Coupe, Eugene Lim, Austin Cherian

## Problem definition

##### Does AWS’s Comprehend have inherent political bias? 


## Abstract 
AWS Comprehend is a natural language processing (NLP) service provided by Amazon Web Services (AWS). It allows users to extract insights and meaning from unstructured text using machine learning algorithms. Two important key functions amongst a host of others of Comprehend are entity recognition, the ability to identify entities such as people, places, and organizations in a given text, and sentiment analysis, the ability to analyze the sentiment of a given text as either positive, negative, or neutral. In this study, we aim to investigate whether AWS Comprehend exhibits inherent political bias in its sentiment analysis feature. Through a regression analysis of six different phrases, entities alligned with the Democratic party appear to receive lower negative sentiment scores than entities alligned with the Republican party.

## Background 
The increasing use of artificial intelligence and machine learning in various aspects of society has raised concerns about political bias in computer algorithms. This bias can shape public opinion and influence people's beliefs and behaviors. For example, search engine result bias has been a significant problem in society, as it can shape public opinion and influence people's beliefs and behaviors. According to a study by Pew Research Center, about 93% of online experiences begin with a search engine, and search engine results can have a significant impact on people's perceptions of various issues. However, there have been concerns about political bias in search engine results. For example, during the 2016 U.S. Presidential election, some people claimed that Google's search results were biased in favor of Hillary Clinton (The Washington Post, 2016). Search engine result bias can also perpetuate stereotypes and limit opportunities for certain groups. According to a report by the Open Society Foundations, search engine algorithms can amplify stereotypes and reinforce prejudice against minority groups, leading to limited access to information and resources (Open Society Foundations, 2018). As search engines become increasingly important sources of information, it's essential that they remain impartial and free from any political bias to ensure that they provide accurate and unbiased information to users.


In addition, social media content moderation bias has become a growing concern in society, as social media platforms like Facebook and Twitter have become a primary source of news and information for many people. According to a survey by Pew Research Center, about 55% of American adults get news from social media (Pew Research Center, 2020). However, there have been concerns about political bias in social media content moderation policies. Some people claim that these platforms unfairly censor conservative voices, while others argue that they allow too much hate speech and misinformation to spread. According to a report by the non-profit organization PEN America, social media platforms have failed to effectively combat hate speech and disinformation, which has led to the spread of dangerous and false information (PEN America, 2019). Social media content moderation bias can also have a chilling effect on free speech, as people may be hesitant to express their opinions or share information for fear of being censored or punished. As social media continues to play a significant role in public discourse, it's important that these platforms implement fair and transparent content moderation policies that are free from political bias.


According to Rozado (2022), ChatGPT is a large language model trained by OpenAI based on the GPT-3.5 architecture, and he attempts to assess ChatGPT's political leanings by feeding it several political orientation tests such as “Pew Research Political Typology Quiz,” “Political Compass Test,” “World’s Smallest Political Quiz,” and the “Political Spectrum Quiz.” The author notes that the quizzes contain questions that cover a range of topics, including economics, social issues, foreign policy, and more, and ChatGPT's responses to the questions were analyzed to determine where it might fall on the political compass. Rozado consistently found across all tests that ChatGPT’s answers to political orientation quizzes were left-leaning. The results of his assessment are detailed in the following visual (Rozado, 2022).
<p align="center">
<img src="https://github.com/joshuajacobs2020/QTM-350---Group-4/raw/main/results.png" width="728" height="800">
</p>
Rozado (2022) acknowledges that ChatGPT is a machine-learning model and, therefore, does not have political beliefs or opinions of its own. However, he notes that the responses generated by ChatGPT suggest that it was "trained on content containing political biases." ChatGPT, like many artificial intelligence systems, is trained on a copious amount of textual data from the internet. Additionally, Rozado (2022) highlights the possibility that "a team of human raters was involved in judging the quality of the model’s answers" and the political bias of the human raters, assuming that they were not a representative sample of the larger public, could have also influenced the political leanings of ChatGPT.

Through further analysis of this study, we found that some of the questions fed to Chat-GPT to measure its political leanings were not very scientific. Rozado (2022) fed objective questions with clear factual answers to determine Chat-GPT’s political leanings. For example, he asks Chat-GPT: “The earth is 6,000-10,000 years old. Agree or Disagree.” He proceeds to use the answer to this objective question that has clear empirical and scientific support to determine whether the AI is left or right-leaning.

Overall, the article provides an interesting insight into how machine learning models like ChatGPT might reflect the political views present in the data they were trained on. However, the author also notes that this does not necessarily mean that ChatGPT or other machine learning models have political beliefs of their own. When algorithms are biased, they can have a significant impact on society. It can reinforce existing prejudices and stereotypes, limit opportunities for certain groups, and perpetuate inequality. Biased algorithms can also undermine public trust in institutions that use them, such as government agencies, law enforcement, and media outlets. As algorithms become more prevalent in our daily lives, it's important that they are transparent and accountable and that measures are taken to ensure that they are free from any political bias. The findings of Rozado (2022) raise the question of whether other AIs like AWS Comprehend also have political leanings. As a result, we aim to do a similar analysis of AWS’s ML, Comprehend, to see if it contains any political bias as well. 

## Hypothesis
We hypothesize that AWS Comprehend would have some inherent political bias because given the ChatGPT study performed by Rozado in 2022 which found that ChatGPT consistently produced left-leaning responses to poltical orientation quizzes, we think it is reasonable to assume that AWS Comprehend would produce similarly biased responses. ChatGPT has been trained on a massive amount of data and contains more than 175 billion parameters, and we believe that if training on a huge amount of data still yielded politically biased responses, that the same will be the case for AWS Comprehend.

## Methods
We imported a data file from Every Politician (https://everypolitician.org/united-states-of-america/senate/download.html) that contains the names of every Congress member in the 116th Congress (2019) and information about them (social media accounts, state they represent, gender, etc.) 
We cleaned the data only to include the names of Congressmen, their listed political party affiliations, and their gender. 
We then converted the political parties into binary (Democrat=1, Republican=0) and gender into binary (female=1, male=0). 

AWS Comprehend generates a positive, negative, mixed, neutral, and overall sentiment score given an inputted text. We generated six phrases for this project with different overall sentiments: two positive phrases, two negative phrases, and two neutral phrases: 

**Neutral Phrases**
* "claims they want to protect children from the dangers of social media."
* "introduced new gun control legislation to Congress."

**Positive Phrases**
* "is a really great politican"
* "donated millions of dollars to charity!"

**Negative Phrases**
* "want to take away human rights."
* "commited tax fraud"

The baseline sentiment of these phrases was generated by inserting the neutral entity "they" before each phrase and calling Comprehend to analyze the overall sentiment of the resulting sentences.

Every Senator's name will be paired with each of these phrases to create full sentences to be analyzed by Comprehend. For example, we created the sentence "Amy Klobuchar claims they want to protect children from the dangers of social media" using Senator Amy Klobuchar as the entity. We can then observe how changing the Senator's name for these phrases may change the sentiment scores. 

Additionally, we generated a list of 98 random names to be paired with these phrases and analyzed by Comprehend. These names come from a public dataset created by the [Social Security Administration](https://console.cloud.google.com/marketplace/details/social-security-administration/us-names?project=qtm350). This dataset contains all names from Social Security card applications for births that occurred in the United States after 1879. These random names can help us understand whether the politicans' names generate greater biases in sentiment scores due to their party affiliation or by random chance.

Lastly, we can also run a sentiment analysis on the entity names alone. If Comprehend generates biased results when generating sentiment scores relating to political figures, it's possible that Comprehend could detect the names alone to be positive or negative depending on their political affiliation.

## Analysis
These box and whisker plots show the distribution of neutral, positive, and negative scores across all phrases.
<p align="center">
<img src="https://github.com/joshuajacobs2020/QTM-350---Group-4/raw/main/Neutral%20Scores.png" width="781" height="551">
<img src="https://github.com/joshuajacobs2020/QTM-350---Group-4/raw/main/Positive%20Scores.png" width="781" height="551">
<img src="https://github.com/joshuajacobs2020/QTM-350---Group-4/raw/main/Negative%20Scores.png" width="781" height="551">
</p>
On average, Republicans appear to score about .2% higher positive scores and about 1.2% higher negative scores than Democrats. By contrast, Democrats tended to score about 2% higher neutral scores. This suggests that there could be a potential bias in Comprehend against Republicans; however, these differences are very small. It is difficult to tell whether or not these discrepancies occur through random error or through a true bias within Comprehend. We may be able to better understand the relationship with political affiliation and sentiment scores if we employ linear regression.

#### Regression Results
Below are four regression tables for each phrase type. Each table contains four regression models which take the following form:

(1) $NonNeutralScores_i = \beta_0 + \beta_1 Democrat_i + \beta_2 Gender_i + \varepsilon_i$ 

(2) $PositiveScores_i = \beta_0 + \beta_1 Democrat_i + \beta_2 Gender_i + \varepsilon_i$ 

(3) $MixedScores_i = \beta_0 + \beta_1 Democrat_i + \beta_2 Gender_i + \varepsilon_i$ 

(4) $NegativeScores_i = \beta_0 + \beta_1 Democrat_i + \beta_2 Gender_i + \varepsilon_i$ 

Each of these models were computed with robust standard errors.

**Table 1: Negative Phrases**

![Table 1](https://quicklatex.com/cache3/ff/ql_a74a31d5e59f0f057f4ee9de0dae5fff_l3.png)

**Table 2: Positive Phrases**

![Table 2](https://quicklatex.com/cache3/2e/ql_ac09492b88f7afe371599438d4c5832e_l3.png)

**Table 3: Neutral Phrases**

![Table 3](https://quicklatex.com/cache3/73/ql_0748f6b780d0086a43a0c3b39fe17273_l3.png)

**Table 4: Names Only**

![Table 4](https://quicklatex.com/cache3/85/ql_c80cebb1f18759fe4c577a749be7cb85_l3.png)

As our box and whisker plots suggested, Republicans scored small but statistically significant higher positive sentiment scores on positive phrases by about 0.3%. However, they scored 7.8% higher negative sentiment scores on negative phrases. These relationships were both significant at a 99% confidence interval (p<.01). For the neutral and blank phrases, there was no statistically significant relationship between party affiliation and sentiment scores. 

While these percentages may seem small, it is important to note that the "overall" sentiment category only reports the highest sentiment score category. This means that if a phrase by itself has high neutral and negative sentiment scores, the overall sentiment would be reported as "negative" more often for Republicans than Democrats. For example, the overall sentiment of the phrase "wants to take away human rights" was "negative" 51% of the time for Republicans but only 15.2% of the time for Democrats. This suggests that the small biases in negative sentiment scores can potentially result in a much more consequential bias on overall sentiments.

Gender was not a statistically significant predictor of sentiment scores in any of the phrase groups. Resultingly, there is little evidence of a gender bias in AWS Comprehend.

#### Comparison with Random Names Group
Another way to better understand Comprehend's potential bias is to use our list of random names as a control group. It is possible that different names generate different sentiment scores solely due to random error. We can then compare the sentiment scores of the Democratic and Republican politicans with this collection of random, non-partisan names.

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th>Group</th>
      <th>Negative</th>
      <th>Positive</th>
      <th>NonNeutral</th>
      <th>Mixed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Democrat</th>
      <td>0.174217</td>
      <td>0.160817</td>
      <td>0.336725</td>
      <td>0.001692</td>
    </tr>
    <tr>
      <th>Non-Partisan</th>
      <td>0.204476</td>
      <td>0.180079</td>
      <td>0.393740</td> 
      <td>0.009185</td>
    </tr>
    <tr>
      <th>Republican</th>
      <td>0.200530</td>
      <td>0.164817</td>
      <td>0.367347</td>
      <td>0.002000</td>
    </tr>
  </tbody>
</table>

As we can see from this table, average negative scores for Republicans were very close with the non-partisan group scores. Democrats, however, tended to score lower negative scores than the control group. This suggests that Comprehend may be biased in giving Democrats lower negative scores, not by giving Republicans higher negative scores. Interestingly, Comprehend detected much lower mixed scores for the Democratic and Republican groups than the control. It is possible that since the politician names are known figures, there is less ambiguity in whether their sentiment scores are positive, negative, or neutral.

## Conclusions
Our box and whisker plots suggest that Republicans have higher positive and negative sentiment scores while Democrats have higher neutral sentiment scores, but we needed to employ linear regression to determine if these relationships are statistically significant. The linear regression results confirm our suspicion that Republicans have higher positive and negative scores. In fact, this relationship between Republicans and both positive and negative scores is statistically significant at a p < 0.01 significance level. However, linear regression does not uncover a statistically significant relationship between Democrats and neutral scores. Moreover, there is no significant relationship to be found between political party affiliation and blank or neutral phrases or between gender and sentiment scores. 

In short, we can conclude that AWS Comprehend is both unbiased for gender and politically unbiased for neutral and blank phrases, but that it exhibits small, but statistically significant political bias for positive and negative phrases. These biases could potentially cause significant differences in overall sentiment sccores, so overall sentiment scores should be scrutinized if the text involves political entities. 

### References:
Pew Research Center. (2021). About Pew Research Center. Retrieved from https://www.pewresearch.org/about/

The Washington Post. (2016). Why Google is being accused of pro-Clinton bias. Retrieved from https://www.washingtonpost.com/news/the-switch/wp/2016/06/10/why-google-is-being-accused-of-pro-clinton-bias/

Open Society Foundations. (2018). How search engines reinforce racism. Retrieved from https://www.opensocietyfoundations.org/voices/how-search-engines-reinforce-racism

Pew Research Center. (2020). Social media usage in 2020. Retrieved from https://www.pewresearch.org/internet/2021/04/07/social-media-use-in-2021/

PEN America. (2019). False equation: the dangers of “free speech absolutism”. Retrieved from https://pen.org/false-equation-the-dangers-of-free-speech-absolutism/
Rozado, D. (2022, December 13). Where Does ChatGPT Fall on the Political Compass? Reason. Retrieved from https://reason.com/2022/12/13/where-does-chatgpt-fall-on-the-political-compass/
